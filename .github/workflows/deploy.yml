name: Deploy Super Mario Game to EKS

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      confirm_destroy:
        description: "Type 'yes' to confirm Terraform destroy"
        required: true
  delete:

jobs:
  deploy:
    runs-on: self-hosted

    env:
      AWS_REGION: us-east-1
      ECR_REPOSITORY: supermario-game
      CLUSTER_NAME: super-mario-eks
      TERRAFORM_DIR: terraform-files

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      # Fix 1: Ensure AWS CLI is properly installed and in PATH
      - name: Install and configure AWS CLI
        run: |
          echo "Checking for AWS CLI..."
          if ! command -v aws &> /dev/null; then
            echo "AWS CLI not found, installing..."
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli
            rm -rf aws awscliv2.zip
          fi
          
          # Verify installation
          aws --version
          
          # Configure AWS credentials
          mkdir -p $HOME/.aws
          cat > $HOME/.aws/credentials << EOF
          [default]
          aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}
          EOF
          
          cat > $HOME/.aws/config << EOF
          [default]
          region=${{ env.AWS_REGION }}
          output=json
          EOF
          
          echo "AWS credentials configured"

      - name: Ensure kubectl is available
        run: |
          if ! command -v kubectl &> /dev/null; then
            echo "kubectl not found, installing..."
            curl -LO "https://dl.k8s.io/release/v1.25.0/bin/linux/amd64/kubectl"
            chmod +x kubectl
            sudo mv kubectl /usr/local/bin/
          fi
          kubectl version --client

      - name: Set up Node.js
        uses: actions/setup-node@v2
        with:
          node-version: '16'

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v1

      # Find Terraform directory
      - name: Locate Terraform directory
        id: find-terraform
        run: |
          if [ -d "${{ env.TERRAFORM_DIR }}" ]; then
            echo "terraform_path=${{ env.TERRAFORM_DIR }}" >> $GITHUB_OUTPUT
          else
            TF_PATH=$(find . -type d -name "terraform-files" | head -n 1)
            if [ -n "$TF_PATH" ]; then
              echo "terraform_path=$TF_PATH" >> $GITHUB_OUTPUT
            else
              echo "ERROR: terraform-files directory not found."
              exit 1
            fi
          fi

      # Initialize Terraform
      - name: Initialize Terraform
        run: |
          cd ${{ steps.find-terraform.outputs.terraform_path }}
          terraform init

      # Fix 2: Handle manual terraform destroy with timeout and error handling
      - name: Manual Terraform destroy
        if: ${{ github.event.inputs.confirm_destroy == 'yes' }}
        run: |
          cd ${{ steps.find-terraform.outputs.terraform_path }}
          
          # First remove node groups to prevent destroy timeouts
          echo "Removing EKS node groups first..."
          terraform state list | grep aws_eks_node_group || echo "No node groups found"
          terraform state list | grep aws_eks_node_group | xargs -r terraform destroy -auto-approve -target
          
          # Then remove EKS cluster
          echo "Removing EKS cluster..."
          terraform state list | grep aws_eks_cluster || echo "No EKS cluster found"
          terraform state list | grep aws_eks_cluster | xargs -r terraform destroy -auto-approve -target
          
          # Finally destroy everything else
          echo "Destroying remaining resources..."
          terraform destroy -auto-approve
          
          echo "Terraform resources destroyed successfully."

      # Exit early if this is just a destroy operation
      - name: Exit after destroy
        if: ${{ github.event.inputs.confirm_destroy == 'yes' }}
        run: |
          echo "Manual destroy operation completed successfully."
          exit 0

      # Check if cluster exists
      - name: Check EKS cluster existence
        id: check-cluster
        run: |
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
          fi

      # Apply Terraform to create infrastructure
      - name: Apply Terraform
        id: terraform-apply
        if: ${{ steps.check-cluster.outputs.cluster_exists == 'false' }}
        run: |
          cd ${{ steps.find-terraform.outputs.terraform_path }}
          terraform apply -auto-approve
          echo "terraform_success=true" >> $GITHUB_OUTPUT

      # Handle Docker and ECR operations
      - name: Check if Docker is available
        id: check-docker
        run: |
          if command -v docker &> /dev/null; then
            echo "docker_available=true" >> $GITHUB_OUTPUT
          else
            echo "docker_available=false" >> $GITHUB_OUTPUT
          fi

      - name: Check if ECR repository exists
        id: check-ecr
        run: |
          if aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            ECR_URI=$(aws ecr describe-repositories --repository-names ${{ env.ECR_REPOSITORY }} --query 'repositories[0].repositoryUri' --output text)
          else
            ECR_URI=$(aws ecr create-repository --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --query 'repository.repositoryUri' --output text)
          fi
          echo "ECR_URI=$ECR_URI" >> $GITHUB_OUTPUT

      # Build and push Docker image
      - name: Build and push Docker image
        id: docker-build
        if: steps.check-docker.outputs.docker_available == 'true'
        run: |
          # Get ECR login token
          ECR_PASSWORD=$(aws ecr get-login-password --region ${{ env.AWS_REGION }})
          ECR_REGISTRY=$(echo "${{ steps.check-ecr.outputs.ECR_URI }}" | cut -d'/' -f1)
          echo "$ECR_PASSWORD" | docker login --username AWS --password-stdin "$ECR_REGISTRY"
          
          # Find Docker directory
          if [ -d "docker" ]; then
            DOCKER_DIR="docker"
          else
            DOCKER_DIR=$(find . -type d -name "docker" | head -n 1)
          fi
          
          if [ -n "$DOCKER_DIR" ]; then
            cd "$DOCKER_DIR"
            docker build -t ${{ env.ECR_REPOSITORY }}:latest . || { echo "docker_success=false" >> $GITHUB_OUTPUT; exit 1; }
            docker tag ${{ env.ECR_REPOSITORY }}:latest ${{ steps.check-ecr.outputs.ECR_URI }}:latest || { echo "docker_success=false" >> $GITHUB_OUTPUT; exit 1; }
            docker push ${{ steps.check-ecr.outputs.ECR_URI }}:latest || { echo "docker_success=false" >> $GITHUB_OUTPUT; exit 1; }
            echo "docker_success=true" >> $GITHUB_OUTPUT
          else
            echo "ERROR: Docker directory not found."
            echo "docker_success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      # Deploy to Kubernetes
      - name: Update kubeconfig and deploy
        id: k8s-deploy
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          
          if [ -d "k8s" ]; then
            K8S_DIR="k8s"
          else
            K8S_DIR=$(find . -type d -name "k8s" | head -n 1)
          fi
          
          if [ -n "$K8S_DIR" ]; then
            cd "$K8S_DIR"
            kubectl apply -f k8s-deployment.yml || { echo "k8s_success=false" >> $GITHUB_OUTPUT; exit 1; }
            
            if [ "${{ steps.check-docker.outputs.docker_available }}" == "true" ] && [ "${{ steps.docker-build.outputs.docker_success }}" == "true" ]; then
              kubectl set image deployment/supermario-game supermario-game=${{ steps.check-ecr.outputs.ECR_URI }}:latest || { echo "k8s_success=false" >> $GITHUB_OUTPUT; exit 1; }
              kubectl rollout status deployment/supermario-game --timeout=180s || { echo "k8s_success=false" >> $GITHUB_OUTPUT; exit 1; }
              echo "k8s_success=true" >> $GITHUB_OUTPUT
            else
              echo "WARNING: Skipping image update due to docker build failure"
              echo "k8s_success=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "ERROR: k8s directory not found."
            echo "k8s_success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      # Fix 3: Improved cleanup on failure with proper EKS resource handling
      - name: Cleanup on failure
        if: failure() || steps.k8s-deploy.outputs.k8s_success == 'false' || steps.docker-build.outputs.docker_success == 'false'
        run: |
          echo "Deployment failed, cleaning up resources..."
          cd ${{ steps.find-terraform.outputs.terraform_path }}
          
          # Step 1: Drain node groups - this helps avoid EKS stuck state
          if kubectl get nodes &>/dev/null; then
            echo "Draining all nodes..."
            kubectl get nodes -o name | xargs -r -I{} kubectl drain {} --delete-emptydir-data --force --ignore-daemonsets
          fi
          
          # Step 2: Delete node groups first using terraform
          echo "Removing EKS node groups..."
          terraform state list | grep aws_eks_node_group || echo "No node groups found"
          terraform state list | grep aws_eks_node_group | xargs -r terraform destroy -auto-approve -target
          
          # Step 3: Delete EKS cluster
          echo "Removing EKS cluster..."
          terraform state list | grep aws_eks_cluster || echo "No EKS cluster found"
          terraform state list | grep aws_eks_cluster | xargs -r terraform destroy -auto-approve -target
          
          # Step 4: Delete everything else
          echo "Destroying remaining resources..."
          terraform destroy -auto-approve -parallelism=10
          
          echo "Resources destroyed successfully after failed deployment."